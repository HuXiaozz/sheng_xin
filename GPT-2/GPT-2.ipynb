{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f91718-73d7-4b49-8e22-c3e5cc572dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9156\n",
      "19989\n",
      "29145\n"
     ]
    }
   ],
   "source": [
    "lu = 250\n",
    "f = open('./250/arabidopsis.csv')\n",
    "ara_data = []\n",
    "ara_label = []\n",
    "rice_data = []\n",
    "rice_label = []\n",
    "non_data=[]\n",
    "non_label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    if(int(tt[1]) == 1):\n",
    "        ara_data.append(tt[0])\n",
    "        ara_label.append(int(tt[1]))\n",
    "    else:\n",
    "        non_data.append(tt[0])\n",
    "        non_label.append(int(tt[1]))\n",
    "\n",
    "lu = 250\n",
    "f = open('./250/rice.csv')\n",
    "\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "   \n",
    "    if(int(tt[1]) == 1):\n",
    "        rice_data.append(tt[0])\n",
    "        rice_label.append(int(tt[1])+1)\n",
    "    else:\n",
    "        non_data.append(tt[0])\n",
    "        non_label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(len(rice_data))\n",
    "print(len(ara_data))\n",
    "print(len(non_data))\n",
    "\n",
    "zong_data = rice_data + ara_data[:9156] + non_data[:9156]\n",
    "zong_label = rice_label +  ara_label[:9156] + non_label[:9156]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# ‰ΩøÁî®zip()ÂáΩÊï∞Â∞Ü‰∏§‰∏™Êï∞ÁªÑÂêàÂπ∂ÊàêÂÖÉÁªÑÁöÑÂàóË°®\n",
    "combined = list(zip(zong_data, zong_label))\n",
    "\n",
    "\n",
    "\n",
    "# ‰ΩøÁî®random.shuffle()ÂáΩÊï∞ÂØπÂêàÂπ∂ÂêéÁöÑÂàóË°®ËøõË°åÊâì‰π±\n",
    "random.shuffle(combined)\n",
    "\n",
    "\n",
    "# ‰ΩøÁî®zip()ÂáΩÊï∞Â∞ÜÊâì‰π±ÂêéÁöÑÂÖÉÁªÑÁöÑÂàóË°®ÊãÜÂàÜÊàê‰∏§‰∏™Êï∞ÁªÑ\n",
    "zong_data, zong_label = zip(*combined)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_label,test_label=train_test_split(zong_data,zong_label,test_size=0.1,random_state=3753,stratify=zong_label)\n",
    "\n",
    "f = open('./train_duofenlei.csv','w')\n",
    "f.write('seq,label\\n')\n",
    "data = []\n",
    "for i in range(len(train_data)):\n",
    "    f.write(train_data[i]+','+str(train_label[i])+'\\n')\n",
    "\n",
    "f = open('./test_duofenlei.csv','w')\n",
    "f.write('seq,label\\n')\n",
    "data = []\n",
    "for i in range(len(test_data)):\n",
    "    f.write(test_data[i]+','+str(test_label[i])+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ba51bc-3301-4449-81db-830267b54770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f8812-280d-4827-9d55-5d1a0f601963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./GPT-2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): CustomWte(\n",
      "      (embedding): Embedding(50257, 768)\n",
      "      (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "    )\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n",
      "********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [00:01<00:00, 2180.45 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 425/425 [00:00<00:00, 2127.22 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/4800 00:03 < 41:12, 1.94 it/s, Epoch 0.03/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ara  + gpt2  + EFPN\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "\n",
    "torch.manual_seed(48)\n",
    "np.random.seed(56)\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑGPT-2Ê®°ÂûãÂíåtokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./GPT-2\")\n",
    "\n",
    "\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"./GPT-2\")\n",
    "\n",
    "# Ë∞ÉÊï¥Ê®°ÂûãÁöÑÂ±ÇÊï∞\n",
    "config.n_layer = 3  # ËÆæÁΩÆ‰∏∫6Â±Ç\n",
    "config.num_labels = 3\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"./GPT-2\",config = config)\n",
    "\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâçwteÂ±ÇÁöÑÊùÉÈáçÁü©ÈòµÂíåembeddingÁª¥Â∫¶\n",
    "current_wte = model.transformer.wte.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# Ëá™ÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑÂµåÂÖ•Â±ÇÔºàÂèØ‰ª•ÊõøÊç¢‰∏∫ÊÇ®ÊÉ≥Ë¶ÅÁöÑÂµåÂÖ•ÊñπÂºèÔºâ\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# ÊõøÊç¢Ê®°ÂûãÁöÑwteÂ±Ç‰∏∫Ëá™ÂÆö‰πâÁöÑÂµåÂÖ•Â±Ç\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.transformer.wte = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"./tools/accuracy.py\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "print(\"********\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./data/train.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "model_name = \"./RBT3/RBT3\"\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# Êï∞ÊçÆÈõÜÂ§ÑÁêÜ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_steps = 4,\n",
    "    seed=4112,\n",
    "    output_dir=\"/hy-tmp/gpt2_EFPN_HBLOSS\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    " data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./test_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)\n",
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20bbf1-2b18-46d1-bf5b-0f634f5c72f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45eb4cae-369b-4687-9947-09de5de393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./gpt2zzz/added_tokens.json. We won't load it.\n",
      "Didn't find file ./gpt2zzz/special_tokens_map.json. We won't load it.\n",
      "loading file ./gpt2zzz/vocab.json\n",
      "loading file ./gpt2zzz/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file ./gpt2zzz/tokenizer_config.json\n",
      "loading configuration file ./gpt2zzz/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./gpt2zzz\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 3,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file ./gpt2zzz/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 3,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./gpt2zzz/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./gpt2zzz were not used when initializing GPT2ForSequenceClassification: ['h.8.mlp.c_proj.bias', 'h.5.attn.bias', 'h.8.mlp.c_fc.bias', 'h.11.attn.c_attn.weight', 'h.5.attn.c_attn.weight', 'h.6.attn.bias', 'h.5.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.6.ln_1.weight', 'h.7.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.11.ln_2.bias', 'h.7.ln_2.weight', 'h.5.mlp.c_proj.bias', 'h.9.ln_2.weight', 'h.9.attn.bias', 'h.9.ln_2.bias', 'h.9.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.8.mlp.c_proj.weight', 'h.10.ln_2.bias', 'h.9.mlp.c_fc.bias', 'h.11.ln_1.weight', 'h.5.attn.c_proj.bias', 'h.5.attn.c_attn.bias', 'h.11.ln_1.bias', 'h.8.mlp.c_fc.weight', 'h.7.attn.c_attn.weight', 'h.7.mlp.c_proj.weight', 'h.10.ln_1.weight', 'h.6.mlp.c_proj.bias', 'h.11.attn.c_proj.weight', 'h.5.mlp.c_fc.weight', 'h.10.ln_2.weight', 'h.10.mlp.c_fc.weight', 'h.8.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.7.ln_1.weight', 'h.10.attn.c_attn.weight', 'h.11.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.5.mlp.c_proj.weight', 'h.11.ln_2.weight', 'h.8.attn.c_proj.weight', 'h.6.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.9.ln_1.bias', 'h.9.mlp.c_fc.weight', 'h.10.mlp.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.5.mlp.c_fc.bias', 'h.7.attn.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.7.mlp.c_fc.weight', 'h.5.ln_2.bias', 'h.6.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.8.ln_2.bias', 'h.7.attn.c_proj.weight', 'h.7.ln_2.bias', 'h.8.attn.c_proj.bias', 'h.10.mlp.c_fc.bias', 'h.9.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_proj.bias', 'h.5.ln_1.weight', 'h.8.attn.bias', 'h.6.ln_2.bias', 'h.11.attn.c_attn.bias', 'h.5.ln_1.bias', 'h.9.attn.c_attn.bias', 'h.6.ln_2.weight', 'h.7.ln_1.bias', 'h.9.mlp.c_proj.weight', 'h.8.ln_2.weight', 'h.7.mlp.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.7.mlp.c_fc.bias', 'h.10.attn.c_proj.bias', 'h.7.attn.bias', 'h.10.attn.c_proj.weight', 'h.9.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.11.attn.bias', 'h.11.mlp.c_proj.weight', 'h.6.mlp.c_fc.weight', 'h.10.attn.bias']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./gpt2zzz and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): CustomWte(\n",
      "      (embedding): Embedding(50257, 768)\n",
      "      (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "    )\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n",
      "********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Using eos_token, but it is not set yet.\n",
      "Assigning [PAD] to the pad_token key of the tokenizer\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27820' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27820/27820 5:40:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.747240</td>\n",
       "      <td>0.661949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.723604</td>\n",
       "      <td>0.687829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.630797</td>\n",
       "      <td>0.736757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.628735</td>\n",
       "      <td>0.742014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.600187</td>\n",
       "      <td>0.763041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.601766</td>\n",
       "      <td>0.774767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.548829</td>\n",
       "      <td>0.780024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.539408</td>\n",
       "      <td>0.785281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.552410</td>\n",
       "      <td>0.791751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.591488</td>\n",
       "      <td>0.771128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.625680</td>\n",
       "      <td>0.778811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.503836</td>\n",
       "      <td>0.806712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.577987</td>\n",
       "      <td>0.789325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.574871</td>\n",
       "      <td>0.794986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.591776</td>\n",
       "      <td>0.793773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.805499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.641222</td>\n",
       "      <td>0.786899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.631165</td>\n",
       "      <td>0.786494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>0.798625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.591516</td>\n",
       "      <td>0.803073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692 (score: 0.8067124949454104).\n",
      "The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2747\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./duofenleimodel/efpngpt2\n",
      "Configuration saved in ./duofenleimodel/efpngpt2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-3.238 ,  3.56  ,  1.027 ],\n",
      "       [-2.576 ,  3.098 ,  1.573 ],\n",
      "       [-3.355 , -2.791 ,  6.55  ],\n",
      "       ...,\n",
      "       [-1.208 , -1.341 ,  2.27  ],\n",
      "       [ 2.494 ,  0.3867, -2.12  ],\n",
      "       [ 4.03  , -1.148 , -2.578 ]], dtype=float16), label_ids=array([1, 1, 2, ..., 2, 1, 0]), metrics={'test_loss': 0.4935366213321686, 'test_accuracy': 0.7986894794321078, 'test_runtime': 40.1317, 'test_samples_per_second': 68.45, 'test_steps_per_second': 4.286})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./duofenleimodel/efpngpt2/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/efpngpt2/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/efpngpt2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# ara  + gpt2  + EFPN\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "\n",
    "torch.manual_seed(48)\n",
    "np.random.seed(56)\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑGPT-2Ê®°ÂûãÂíåtokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2zzz\")\n",
    "\n",
    "\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"./gpt2zzz\")\n",
    "\n",
    "# Ë∞ÉÊï¥Ê®°ÂûãÁöÑÂ±ÇÊï∞\n",
    "config.n_layer = 5  # ËÆæÁΩÆ‰∏∫6Â±Ç\n",
    "config.num_labels = 3\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"./gpt2zzz\",config = config)\n",
    "\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâçwteÂ±ÇÁöÑÊùÉÈáçÁü©ÈòµÂíåembeddingÁª¥Â∫¶\n",
    "current_wte = model.transformer.wte.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# Ëá™ÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑÂµåÂÖ•Â±ÇÔºàÂèØ‰ª•ÊõøÊç¢‰∏∫ÊÇ®ÊÉ≥Ë¶ÅÁöÑÂµåÂÖ•ÊñπÂºèÔºâ\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# ÊõøÊç¢Ê®°ÂûãÁöÑwteÂ±Ç‰∏∫Ëá™ÂÆö‰πâÁöÑÂµåÂÖ•Â±Ç\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.transformer.wte = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"./evaluate/metrics/accuracy/accuracy.py\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "print(\"********\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "model_name = \"./rbt3\"\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# Êï∞ÊçÆÈõÜÂ§ÑÁêÜ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_steps = 4,\n",
    "    seed=4112,\n",
    "    output_dir=\"/hy-tmp/gpt2_EFPN_HBLOSS\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    " data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./test_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)\n",
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b669be-e73e-433f-bbf1-0617994d71ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22248\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a89aa3-a327-40d5-ab73-aeb42a9a3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./duofenleimodel/efpngpt2\n",
      "Configuration saved in ./duofenleimodel/efpngpt2/config.json\n",
      "Model weights saved in ./duofenleimodel/efpngpt2/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/efpngpt2/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/efpngpt2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bee63-006f-41a1-b9f2-8581554e18c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22248/22248 [00:11<00:00, 1939.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2473/2473 [00:01<00:00, 1939.08 examples/s]\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file ./rbt3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./rbt3 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): CustomWte(\n",
      "        (embedding): Embedding(21128, 768)\n",
      "        (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "      )\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11727' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11727/27820 1:13:18 < 1:40:37, 2.67 it/s, Epoch 8.43/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.710100</td>\n",
       "      <td>0.724626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>0.579693</td>\n",
       "      <td>0.763850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.671057</td>\n",
       "      <td>0.749292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.630538</td>\n",
       "      <td>0.756167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.595819</td>\n",
       "      <td>0.771533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.508993</td>\n",
       "      <td>0.800647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.600525</td>\n",
       "      <td>0.771533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.788516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# rbt3 duofenlei\n",
    "\n",
    "#rbt3   ara EFPN\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "model_name = \"./rbt3\" # ÊâÄ‰ΩøÁî®Ê®°Âûã\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# Êï∞ÊçÆÈõÜÂ§ÑÁêÜ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ÊûÑÂª∫ËØÑ‰º∞ÂáΩÊï∞\n",
    "metric = evaluate.load('./evaluate/metrics/accuracy/accuracy.py')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# ËÆ≠ÁªÉÂô®ÈÖçÁΩÆ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâçwteÂ±ÇÁöÑÊùÉÈáçÁü©ÈòµÂíåembeddingÁª¥Â∫¶\n",
    "current_wte = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# Ëá™ÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑÂµåÂÖ•Â±ÇÔºàÂèØ‰ª•ÊõøÊç¢‰∏∫ÊÇ®ÊÉ≥Ë¶ÅÁöÑÂµåÂÖ•ÊñπÂºèÔºâ\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# ÊõøÊç¢Ê®°ÂûãÁöÑwteÂ±Ç‰∏∫Ëá™ÂÆö‰πâÁöÑÂµåÂÖ•Â±Ç\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.bert.embeddings.word_embeddings = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=16,\n",
    "  num_train_epochs=20,\n",
    "  weight_decay=0.01,\n",
    "        save_steps = 4,\n",
    "    seed=4112,\n",
    "  output_dir=\"/hy-tmp/bert3_ara_classification\",\n",
    "  logging_steps=10,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  save_strategy = \"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉ‰∏éËØÑ‰º∞\n",
    "trainer.train()\n",
    "data_file = \"./test_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "trainer.save_model(\"./duofenleimodel/bert3ara_EFPN\")\n",
    "# trainer.evaluate()\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b829b4b-3163-4933-abbd-7765088eadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22248/22248 [00:10<00:00, 2097.33 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2473/2473 [00:01<00:00, 2140.42 examples/s]\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file ./rbt3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./rbt3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./rbt3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): CustomWte(\n",
      "        (embedding): Embedding(21128, 768)\n",
      "        (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "      )\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27820' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27820/27820 2:54:46, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.713200</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.720582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.749093</td>\n",
       "      <td>0.690659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.775576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.580652</td>\n",
       "      <td>0.763850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>0.617078</td>\n",
       "      <td>0.752123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.599748</td>\n",
       "      <td>0.781237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.670981</td>\n",
       "      <td>0.736757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.506806</td>\n",
       "      <td>0.809543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.678833</td>\n",
       "      <td>0.763041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.510538</td>\n",
       "      <td>0.803882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.552421</td>\n",
       "      <td>0.802669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.509588</td>\n",
       "      <td>0.824505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.374300</td>\n",
       "      <td>0.518949</td>\n",
       "      <td>0.811161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.556400</td>\n",
       "      <td>0.635491</td>\n",
       "      <td>0.790133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>0.811565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.632629</td>\n",
       "      <td>0.795390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.551755</td>\n",
       "      <td>0.818439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>0.591888</td>\n",
       "      <td>0.808330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.810756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.604541</td>\n",
       "      <td>0.815609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-12519\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-13910\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-15301\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-16692\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-18083\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-19474\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-20865\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-22256\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-23647\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-25038\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-26429\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-27820\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /hy-tmp/bert3_ara_classification/checkpoint-16692 (score: 0.8245046502224019).\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2747/2747 [00:01<00:00, 1965.35 examples/s]\n",
      "Saving model checkpoint to ./duofenleimodel/bert3ara_EFPN\n",
      "Configuration saved in ./duofenleimodel/bert3ara_EFPN/config.json\n",
      "Model weights saved in ./duofenleimodel/bert3ara_EFPN/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/bert3ara_EFPN/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/bert3ara_EFPN/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2747\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-3.25   ,  2.662  ,  0.8506 ],\n",
      "       [-3.912  ,  3.018  ,  0.8545 ],\n",
      "       [-2.834  , -1.954  ,  4.297  ],\n",
      "       ...,\n",
      "       [-1.667  , -2.219  ,  3.523  ],\n",
      "       [ 0.06824,  0.394  , -0.6245 ],\n",
      "       [ 3.617  , -2.621  , -1.481  ]], dtype=float16), label_ids=array([1, 1, 2, ..., 2, 1, 0]), metrics={'test_loss': 0.5371641516685486, 'test_accuracy': 0.8150709865307608, 'test_runtime': 21.3388, 'test_samples_per_second': 128.733, 'test_steps_per_second': 8.06})\n"
     ]
    }
   ],
   "source": [
    "# rbt3 duofenlei\n",
    "\n",
    "#rbt3   ara EFPN\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "model_name = \"./rbt3\" # ÊâÄ‰ΩøÁî®Ê®°Âûã\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# Êï∞ÊçÆÈõÜÂ§ÑÁêÜ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ÊûÑÂª∫ËØÑ‰º∞ÂáΩÊï∞\n",
    "metric = evaluate.load('./evaluate/metrics/accuracy/accuracy.py')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# ËÆ≠ÁªÉÂô®ÈÖçÁΩÆ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâçwteÂ±ÇÁöÑÊùÉÈáçÁü©ÈòµÂíåembeddingÁª¥Â∫¶\n",
    "current_wte = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# Ëá™ÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑÂµåÂÖ•Â±ÇÔºàÂèØ‰ª•ÊõøÊç¢‰∏∫ÊÇ®ÊÉ≥Ë¶ÅÁöÑÂµåÂÖ•ÊñπÂºèÔºâ\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # Â∞ÜÁª¥Â∫¶Ë∞ÉÊï¥‰∏∫(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# ÊõøÊç¢Ê®°ÂûãÁöÑwteÂ±Ç‰∏∫Ëá™ÂÆö‰πâÁöÑÂµåÂÖ•Â±Ç\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.bert.embeddings.word_embeddings = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=16,\n",
    "  num_train_epochs=20,\n",
    "  weight_decay=0.01,\n",
    "        save_steps = 4,\n",
    "    seed=4112,\n",
    "  output_dir=\"/hy-tmp/bert3_ara_classification\",\n",
    "  logging_steps=10,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  save_strategy = \"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉ‰∏éËØÑ‰º∞\n",
    "trainer.train()\n",
    "data_file = \"./test_duofenlei.csv\" # Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑÔºåÊï∞ÊçÆÈúÄË¶ÅÊèêÂâç‰∏ãËΩΩ\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "trainer.save_model(\"./duofenleimodel/bert3ara_EFPN\")\n",
    "# trainer.evaluate()\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b31261-e1fb-4651-87b6-98f107e43ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
