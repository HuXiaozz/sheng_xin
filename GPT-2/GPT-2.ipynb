{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f91718-73d7-4b49-8e22-c3e5cc572dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9156\n",
      "19989\n",
      "29145\n"
     ]
    }
   ],
   "source": [
    "lu = 250\n",
    "f = open('./250/arabidopsis.csv')\n",
    "ara_data = []\n",
    "ara_label = []\n",
    "rice_data = []\n",
    "rice_label = []\n",
    "non_data=[]\n",
    "non_label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    if(int(tt[1]) == 1):\n",
    "        ara_data.append(tt[0])\n",
    "        ara_label.append(int(tt[1]))\n",
    "    else:\n",
    "        non_data.append(tt[0])\n",
    "        non_label.append(int(tt[1]))\n",
    "\n",
    "lu = 250\n",
    "f = open('./250/rice.csv')\n",
    "\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "   \n",
    "    if(int(tt[1]) == 1):\n",
    "        rice_data.append(tt[0])\n",
    "        rice_label.append(int(tt[1])+1)\n",
    "    else:\n",
    "        non_data.append(tt[0])\n",
    "        non_label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(len(rice_data))\n",
    "print(len(ara_data))\n",
    "print(len(non_data))\n",
    "\n",
    "zong_data = rice_data + ara_data[:9156] + non_data[:9156]\n",
    "zong_label = rice_label +  ara_label[:9156] + non_label[:9156]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# 使用zip()函数将两个数组合并成元组的列表\n",
    "combined = list(zip(zong_data, zong_label))\n",
    "\n",
    "\n",
    "\n",
    "# 使用random.shuffle()函数对合并后的列表进行打乱\n",
    "random.shuffle(combined)\n",
    "\n",
    "\n",
    "# 使用zip()函数将打乱后的元组的列表拆分成两个数组\n",
    "zong_data, zong_label = zip(*combined)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_label,test_label=train_test_split(zong_data,zong_label,test_size=0.1,random_state=3753,stratify=zong_label)\n",
    "\n",
    "f = open('./train_duofenlei.csv','w')\n",
    "f.write('seq,label\\n')\n",
    "data = []\n",
    "for i in range(len(train_data)):\n",
    "    f.write(train_data[i]+','+str(train_label[i])+'\\n')\n",
    "\n",
    "f = open('./test_duofenlei.csv','w')\n",
    "f.write('seq,label\\n')\n",
    "data = []\n",
    "for i in range(len(test_data)):\n",
    "    f.write(test_data[i]+','+str(test_label[i])+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ba51bc-3301-4449-81db-830267b54770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f8812-280d-4827-9d55-5d1a0f601963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./GPT-2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): CustomWte(\n",
      "      (embedding): Embedding(50257, 768)\n",
      "      (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "    )\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n",
      "********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3825/3825 [00:01<00:00, 2180.45 examples/s]\n",
      "Map: 100%|██████████| 425/425 [00:00<00:00, 2127.22 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/4800 00:03 < 41:12, 1.94 it/s, Epoch 0.03/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ara  + gpt2  + EFPN\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子\n",
    "\n",
    "torch.manual_seed(48)\n",
    "np.random.seed(56)\n",
    "\n",
    "# 加载预训练的GPT-2模型和tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./GPT-2\")\n",
    "\n",
    "\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"./GPT-2\")\n",
    "\n",
    "# 调整模型的层数\n",
    "config.n_layer = 3  # 设置为6层\n",
    "config.num_labels = 3\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"./GPT-2\",config = config)\n",
    "\n",
    "\n",
    "# 获取当前wte层的权重矩阵和embedding维度\n",
    "current_wte = model.transformer.wte.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# 自定义一个新的嵌入层（可以替换为您想要的嵌入方式）\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # 将维度调整为(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# 替换模型的wte层为自定义的嵌入层\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.transformer.wte = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"./tools/accuracy.py\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "print(\"********\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./data/train.csv\" # 数据文件路径，数据需要提前下载\n",
    "model_name = \"./RBT3/RBT3\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# 数据集处理\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_steps = 4,\n",
    "    seed=4112,\n",
    "    output_dir=\"/hy-tmp/gpt2_EFPN_HBLOSS\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    " data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./test_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)\n",
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20bbf1-2b18-46d1-bf5b-0f634f5c72f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45eb4cae-369b-4687-9947-09de5de393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./gpt2zzz/added_tokens.json. We won't load it.\n",
      "Didn't find file ./gpt2zzz/special_tokens_map.json. We won't load it.\n",
      "loading file ./gpt2zzz/vocab.json\n",
      "loading file ./gpt2zzz/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file ./gpt2zzz/tokenizer_config.json\n",
      "loading configuration file ./gpt2zzz/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./gpt2zzz\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 3,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file ./gpt2zzz/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 3,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./gpt2zzz/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./gpt2zzz were not used when initializing GPT2ForSequenceClassification: ['h.8.mlp.c_proj.bias', 'h.5.attn.bias', 'h.8.mlp.c_fc.bias', 'h.11.attn.c_attn.weight', 'h.5.attn.c_attn.weight', 'h.6.attn.bias', 'h.5.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.6.ln_1.weight', 'h.7.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.11.ln_2.bias', 'h.7.ln_2.weight', 'h.5.mlp.c_proj.bias', 'h.9.ln_2.weight', 'h.9.attn.bias', 'h.9.ln_2.bias', 'h.9.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.8.mlp.c_proj.weight', 'h.10.ln_2.bias', 'h.9.mlp.c_fc.bias', 'h.11.ln_1.weight', 'h.5.attn.c_proj.bias', 'h.5.attn.c_attn.bias', 'h.11.ln_1.bias', 'h.8.mlp.c_fc.weight', 'h.7.attn.c_attn.weight', 'h.7.mlp.c_proj.weight', 'h.10.ln_1.weight', 'h.6.mlp.c_proj.bias', 'h.11.attn.c_proj.weight', 'h.5.mlp.c_fc.weight', 'h.10.ln_2.weight', 'h.10.mlp.c_fc.weight', 'h.8.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.7.ln_1.weight', 'h.10.attn.c_attn.weight', 'h.11.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.5.mlp.c_proj.weight', 'h.11.ln_2.weight', 'h.8.attn.c_proj.weight', 'h.6.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.9.ln_1.bias', 'h.9.mlp.c_fc.weight', 'h.10.mlp.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.5.mlp.c_fc.bias', 'h.7.attn.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.7.mlp.c_fc.weight', 'h.5.ln_2.bias', 'h.6.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.8.ln_2.bias', 'h.7.attn.c_proj.weight', 'h.7.ln_2.bias', 'h.8.attn.c_proj.bias', 'h.10.mlp.c_fc.bias', 'h.9.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_proj.bias', 'h.5.ln_1.weight', 'h.8.attn.bias', 'h.6.ln_2.bias', 'h.11.attn.c_attn.bias', 'h.5.ln_1.bias', 'h.9.attn.c_attn.bias', 'h.6.ln_2.weight', 'h.7.ln_1.bias', 'h.9.mlp.c_proj.weight', 'h.8.ln_2.weight', 'h.7.mlp.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.7.mlp.c_fc.bias', 'h.10.attn.c_proj.bias', 'h.7.attn.bias', 'h.10.attn.c_proj.weight', 'h.9.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.11.attn.bias', 'h.11.mlp.c_proj.weight', 'h.6.mlp.c_fc.weight', 'h.10.attn.bias']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./gpt2zzz and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): CustomWte(\n",
      "      (embedding): Embedding(50257, 768)\n",
      "      (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "    )\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n",
      "********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Using eos_token, but it is not set yet.\n",
      "Assigning [PAD] to the pad_token key of the tokenizer\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27820' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27820/27820 5:40:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.747240</td>\n",
       "      <td>0.661949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.723604</td>\n",
       "      <td>0.687829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.630797</td>\n",
       "      <td>0.736757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.628735</td>\n",
       "      <td>0.742014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.600187</td>\n",
       "      <td>0.763041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.601766</td>\n",
       "      <td>0.774767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.548829</td>\n",
       "      <td>0.780024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.539408</td>\n",
       "      <td>0.785281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.552410</td>\n",
       "      <td>0.791751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.591488</td>\n",
       "      <td>0.771128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.625680</td>\n",
       "      <td>0.778811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.503836</td>\n",
       "      <td>0.806712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.577987</td>\n",
       "      <td>0.789325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.574871</td>\n",
       "      <td>0.794986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.591776</td>\n",
       "      <td>0.793773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.805499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.641222</td>\n",
       "      <td>0.786899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.631165</td>\n",
       "      <td>0.786494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>0.798625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.591516</td>\n",
       "      <td>0.803073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-11128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-12519/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-13910/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-15301/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-18083/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-19474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-20865/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-22256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-23647/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-25038/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-26429/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820\n",
      "Configuration saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/config.json\n",
      "Model weights saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-27820/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /hy-tmp/gpt2_EFPN_HBLOSS/checkpoint-16692 (score: 0.8067124949454104).\n",
      "The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2747\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./duofenleimodel/efpngpt2\n",
      "Configuration saved in ./duofenleimodel/efpngpt2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-3.238 ,  3.56  ,  1.027 ],\n",
      "       [-2.576 ,  3.098 ,  1.573 ],\n",
      "       [-3.355 , -2.791 ,  6.55  ],\n",
      "       ...,\n",
      "       [-1.208 , -1.341 ,  2.27  ],\n",
      "       [ 2.494 ,  0.3867, -2.12  ],\n",
      "       [ 4.03  , -1.148 , -2.578 ]], dtype=float16), label_ids=array([1, 1, 2, ..., 2, 1, 0]), metrics={'test_loss': 0.4935366213321686, 'test_accuracy': 0.7986894794321078, 'test_runtime': 40.1317, 'test_samples_per_second': 68.45, 'test_steps_per_second': 4.286})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./duofenleimodel/efpngpt2/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/efpngpt2/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/efpngpt2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# ara  + gpt2  + EFPN\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子\n",
    "\n",
    "torch.manual_seed(48)\n",
    "np.random.seed(56)\n",
    "\n",
    "# 加载预训练的GPT-2模型和tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2zzz\")\n",
    "\n",
    "\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"./gpt2zzz\")\n",
    "\n",
    "# 调整模型的层数\n",
    "config.n_layer = 5  # 设置为6层\n",
    "config.num_labels = 3\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"./gpt2zzz\",config = config)\n",
    "\n",
    "\n",
    "# 获取当前wte层的权重矩阵和embedding维度\n",
    "current_wte = model.transformer.wte.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# 自定义一个新的嵌入层（可以替换为您想要的嵌入方式）\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # 将维度调整为(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# 替换模型的wte层为自定义的嵌入层\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.transformer.wte = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"./evaluate/metrics/accuracy/accuracy.py\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "print(\"********\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "model_name = \"./rbt3\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# 数据集处理\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_steps = 4,\n",
    "    seed=4112,\n",
    "    output_dir=\"/hy-tmp/gpt2_EFPN_HBLOSS\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    " data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "data_file = \"./test_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)\n",
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b669be-e73e-433f-bbf1-0617994d71ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22248\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a89aa3-a327-40d5-ab73-aeb42a9a3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./duofenleimodel/efpngpt2\n",
      "Configuration saved in ./duofenleimodel/efpngpt2/config.json\n",
      "Model weights saved in ./duofenleimodel/efpngpt2/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/efpngpt2/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/efpngpt2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./duofenleimodel/efpngpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bee63-006f-41a1-b9f2-8581554e18c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 22248/22248 [00:11<00:00, 1939.41 examples/s]\n",
      "Map: 100%|██████████| 2473/2473 [00:01<00:00, 1939.08 examples/s]\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file ./rbt3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./rbt3 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): CustomWte(\n",
      "        (embedding): Embedding(21128, 768)\n",
      "        (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "      )\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11727' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11727/27820 1:13:18 < 1:40:37, 2.67 it/s, Epoch 8.43/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.710100</td>\n",
       "      <td>0.724626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>0.579693</td>\n",
       "      <td>0.763850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.671057</td>\n",
       "      <td>0.749292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.630538</td>\n",
       "      <td>0.756167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.595819</td>\n",
       "      <td>0.771533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.508993</td>\n",
       "      <td>0.800647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.600525</td>\n",
       "      <td>0.771533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.788516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# rbt3 duofenlei\n",
    "\n",
    "#rbt3   ara EFPN\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "model_name = \"./rbt3\" # 所使用模型\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# 数据集处理\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构建评估函数\n",
    "metric = evaluate.load('./evaluate/metrics/accuracy/accuracy.py')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 训练器配置\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 获取当前wte层的权重矩阵和embedding维度\n",
    "current_wte = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# 自定义一个新的嵌入层（可以替换为您想要的嵌入方式）\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # 将维度调整为(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# 替换模型的wte层为自定义的嵌入层\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.bert.embeddings.word_embeddings = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=16,\n",
    "  num_train_epochs=20,\n",
    "  weight_decay=0.01,\n",
    "        save_steps = 4,\n",
    "    seed=4112,\n",
    "  output_dir=\"/hy-tmp/bert3_ara_classification\",\n",
    "  logging_steps=10,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  save_strategy = \"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# 训练与评估\n",
    "trainer.train()\n",
    "data_file = \"./test_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "trainer.save_model(\"./duofenleimodel/bert3ara_EFPN\")\n",
    "# trainer.evaluate()\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b829b4b-3163-4933-abbd-7765088eadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file ./rbt3/vocab.txt\n",
      "loading file ./rbt3/tokenizer.json\n",
      "loading file ./rbt3/added_tokens.json\n",
      "loading file ./rbt3/special_tokens_map.json\n",
      "loading file ./rbt3/tokenizer_config.json\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 22248/22248 [00:10<00:00, 2097.33 examples/s]\n",
      "Map: 100%|██████████| 2473/2473 [00:01<00:00, 2140.42 examples/s]\n",
      "loading configuration file ./rbt3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file ./rbt3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./rbt3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./rbt3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22248\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): CustomWte(\n",
      "        (embedding): Embedding(21128, 768)\n",
      "        (conv1d1): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d2): Conv1d(384, 767, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (conv1d3): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (conv1d4): Conv1d(768, 768, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "      )\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27820' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27820/27820 2:54:46, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.713200</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.720582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.749093</td>\n",
       "      <td>0.690659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.775576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.580652</td>\n",
       "      <td>0.763850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>0.617078</td>\n",
       "      <td>0.752123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.599748</td>\n",
       "      <td>0.781237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.670981</td>\n",
       "      <td>0.736757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.506806</td>\n",
       "      <td>0.809543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.678833</td>\n",
       "      <td>0.763041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.510538</td>\n",
       "      <td>0.803882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.552421</td>\n",
       "      <td>0.802669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.509588</td>\n",
       "      <td>0.824505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.374300</td>\n",
       "      <td>0.518949</td>\n",
       "      <td>0.811161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.556400</td>\n",
       "      <td>0.635491</td>\n",
       "      <td>0.790133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>0.811565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.632629</td>\n",
       "      <td>0.795390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.551755</td>\n",
       "      <td>0.818439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>0.591888</td>\n",
       "      <td>0.808330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.810756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.604541</td>\n",
       "      <td>0.815609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-1391\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-1391/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-2782\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-2782/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-4173\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-4173/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-5564\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-5564/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-6955\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-6955/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-8346\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-8346/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-9737\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-9737/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-11128\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-11128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-12519\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-12519/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-13910\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-13910/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-15301\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-15301/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-16692\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-16692/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-18083\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-18083/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-19474\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-19474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-20865\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-20865/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-22256\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-22256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-23647\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-23647/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-25038\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-25038/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-26429\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-26429/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /hy-tmp/bert3_ara_classification/checkpoint-27820\n",
      "Configuration saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/config.json\n",
      "Model weights saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/pytorch_model.bin\n",
      "tokenizer config file saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/tokenizer_config.json\n",
      "Special tokens file saved in /hy-tmp/bert3_ara_classification/checkpoint-27820/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /hy-tmp/bert3_ara_classification/checkpoint-16692 (score: 0.8245046502224019).\n",
      "Map: 100%|██████████| 2747/2747 [00:01<00:00, 1965.35 examples/s]\n",
      "Saving model checkpoint to ./duofenleimodel/bert3ara_EFPN\n",
      "Configuration saved in ./duofenleimodel/bert3ara_EFPN/config.json\n",
      "Model weights saved in ./duofenleimodel/bert3ara_EFPN/pytorch_model.bin\n",
      "tokenizer config file saved in ./duofenleimodel/bert3ara_EFPN/tokenizer_config.json\n",
      "Special tokens file saved in ./duofenleimodel/bert3ara_EFPN/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2747\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-3.25   ,  2.662  ,  0.8506 ],\n",
      "       [-3.912  ,  3.018  ,  0.8545 ],\n",
      "       [-2.834  , -1.954  ,  4.297  ],\n",
      "       ...,\n",
      "       [-1.667  , -2.219  ,  3.523  ],\n",
      "       [ 0.06824,  0.394  , -0.6245 ],\n",
      "       [ 3.617  , -2.621  , -1.481  ]], dtype=float16), label_ids=array([1, 1, 2, ..., 2, 1, 0]), metrics={'test_loss': 0.5371641516685486, 'test_accuracy': 0.8150709865307608, 'test_runtime': 21.3388, 'test_samples_per_second': 128.733, 'test_steps_per_second': 8.06})\n"
     ]
    }
   ],
   "source": [
    "# rbt3 duofenlei\n",
    "\n",
    "#rbt3   ara EFPN\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_file = \"./train_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "model_name = \"./rbt3\" # 所使用模型\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"].train_test_split(0.1)\n",
    "\n",
    "# 数据集处理\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构建评估函数\n",
    "metric = evaluate.load('./evaluate/metrics/accuracy/accuracy.py')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 训练器配置\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 获取当前wte层的权重矩阵和embedding维度\n",
    "current_wte = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_dim = current_wte.size(1)\n",
    "\n",
    "# 自定义一个新的嵌入层（可以替换为您想要的嵌入方式）\n",
    "class CustomWte(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomWte, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d1 = nn.Conv1d(embedding_dim, 384, kernel_size=3, padding=1)\n",
    "        self.conv1d2 = nn.Conv1d(384, embedding_dim-1, kernel_size=3, padding=1)\n",
    "        self.conv1d3 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=17, padding=8)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings.transpose(1, 2)  # 将维度调整为(batch_size, emb_size, seq_len)\n",
    "        conv_output1 = self.conv1d1(embeddings)\n",
    "\n",
    "        # conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output1 = self.conv1d2(conv_output1)\n",
    "        conv_output1 = conv_output1.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "\n",
    "        conv_output2 = self.conv1d3(embeddings)\n",
    "        \n",
    "        conv_output2 = conv_output2.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output2 = F.max_pool1d(conv_output2, kernel_size=conv_output2.size(2))\n",
    "\n",
    "        conv_output3 = self.conv1d4(embeddings)\n",
    "        conv_output3 = conv_output3.transpose(1, 2)  # 将维度调整为(batch_size, seq_len, emb_size)\n",
    "        conv_output3 = F.max_pool1d(conv_output3, kernel_size=conv_output3.size(2))\n",
    "        \n",
    "        conv_output4 = conv_output2 + conv_output3\n",
    "        conv_output5 = torch.cat([conv_output1,conv_output4],dim = 2)\n",
    "  \n",
    "        return conv_output5\n",
    "\n",
    "# 替换模型的wte层为自定义的嵌入层\n",
    "custom_wte = CustomWte(tokenizer.vocab_size, embedding_dim)\n",
    "model.bert.embeddings.word_embeddings = custom_wte\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=16,\n",
    "  num_train_epochs=20,\n",
    "  weight_decay=0.01,\n",
    "        save_steps = 4,\n",
    "    seed=4112,\n",
    "  output_dir=\"/hy-tmp/bert3_ara_classification\",\n",
    "  logging_steps=10,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  save_strategy = \"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# 训练与评估\n",
    "trainer.train()\n",
    "data_file = \"./test_duofenlei.csv\" # 数据文件路径，数据需要提前下载\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=data_file)\n",
    "dataset = dataset.filter(lambda x: x[\"seq\"] is not None)\n",
    "datasets = dataset[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "def process_function(examples):\n",
    "    for i in range(len(examples['seq'])):\n",
    "        examples['seq'][i] = ' '.join(list(examples['seq'][i]))\n",
    "    tokenized_examples = tokenizer(examples[\"seq\"], max_length=500, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "test_datasets = datasets.map(process_function, batched=True)\n",
    "\n",
    "trainer.save_model(\"./duofenleimodel/bert3ara_EFPN\")\n",
    "# trainer.evaluate()\n",
    "predictions = trainer.predict(test_datasets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b31261-e1fb-4651-87b6-98f107e43ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
